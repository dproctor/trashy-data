---
title: Impact of Clean Up The City on sidewalk cleanliness
format:
  html:
    embed-resources: true
---

We begin by joining the two datasets published by DPW(<https://data.sfgov.org/City-Infrastructure/DPW-Street-Sidewalk-Evaluation-Results-CY22/fsqv-4vqv/about_data>, <https://data.sfgov.org/City-Infrastructure/Street-Sidewalk-Maintenance-Standards-Results-Jan-/qya8-uhsz/about_data>), as neither is a proper subset of the other.

# Observations

```{python}
#| title: Look at schema
#| echo: false
#| output: false
import os
import polars as pl

with pl.Config(tbl_cols=-1, tbl_width_chars=1000):
  print(pl.read_csv("data/DPW_Street___Sidewalk_Evaluation_Results__CY22_20241122.csv").describe())
  print(pl.read_csv(os.environ["TRASH_AUDIT_DATA"]).describe())
```

```
Cleanliness scale

1 = None
2 = A few traces
3 = More than a few traces, but no accumulation
4 = Distributed litter with some accumulation
5 = Widespread litter with significant accumulation.
```

```{python}
#| title: Join old and new datasets
observations = (
  pl.concat([
    pl.read_csv("data/DPW_Street___Sidewalk_Evaluation_Results__CY22_20241122.csv").select(
        pl.col("ObjectID").alias("object_id"),
        pl.col("Route ID:").alias("route_id"),
        pl.col("Select the statement that best describes the amount and distribution of litter on the sidewalk.").alias("sidewalk_litter"),
        pl.col("CreationDate").str.to_datetime("%m/%d/%Y %I:%M:%S %p").alias("evaluation_date"),
        pl.col("the_geom").replace("POINT (0 0)", None).alias("geometry"),
        pl.col("Route Location:").alias("route_location"),
        pl.lit("old").alias("dataset"),
    ),
    pl.read_csv(os.environ["TRASH_AUDIT_DATA"]).select(
        pl.col("ObjectID").alias("object_id"),
        pl.col("Route ID:").alias("route_id"),
        pl.col("Select the statement that best describes the amount and distribution of litter on the sidewalk.").alias("sidewalk_litter"),
        pl.col("CreationDate").str.to_datetime("%m/%d/%Y %I:%M:%S %p").alias("evaluation_date"),
        pl.lit(None).alias("geometry"), # missing precise location data :/
        pl.col("Route Location:").alias("route_location"),
        pl.lit("new").alias("dataset"),
    )
  ])
  .unique(subset=["object_id", "route_id"], keep="first")
)

observations = observations.join(
  pl.read_csv("data/coordinates.csv").select(
    pl.col("Route ID:").alias("route_id"),
    pl.format("POINT ({} {})", pl.col("X (Longitude)"), pl.col("Y (Latitude)")).alias("geometry"),
  ),
  on=["route_id"], how="left"
).with_columns(
  pl.coalesce(pl.col(["geometry", "geometry_right"])).alias("geometry")
).drop("geometry_right")

observations = observations.with_columns(
  pl.col("geometry").str.extract(r"POINT \(([^ ]+)").alias("lon").cast(pl.Float32),
  pl.col("geometry").str.extract(r"([^ ]+)\)").alias("lat").cast(pl.Float32),
)

observations.describe()

# data.sort("evaluation_date").write_csv("/tmp/dups.csv")
```

We see that there is decent support in the time period that Clean Up the City has been active.

```{python}
import matplotlib.pyplot as plt
import seaborn as sns

plt.xticks(rotation=30)
sns.histplot(data=observations, x="evaluation_date")
```

# Cleanups

We import a record of the cleanup times and locations

- export from trashy postgres
- manual mapping from host location -> coordinates

```SQL
SELECT neighborhood, location, time, case when override_participant_count > 0 then override_participant_count else count end
FROM (
SELECT neighborhood, location, time, COUNT(*) as count, AVG(override_participant_count) as override_participant_count
FROM (
    SELECT email, first_name, last_name, time, neighborhood, location, override_participant_count
    FROM event_participants
    FULL OUTER JOIN (
        SELECT events.id as id, events.time, cleanups.neighborhood, cleanups.location, events.override_participant_count
        FROM events
        JOIN cleanups ON events.cleanup_id = cleanups.id
    ) events_and_cleanups ON event_participants.event_id = events_and_cleanups.id
) joined
GROUP BY neighborhood, location, time
) as raw
```

```SQL
SELECT DISTINCT location, neighborhood
FROM cleanups

───────────────────────────────────────────────────
       location               neighborhood
───────────────────────────────────────────────────
1   │  McCoppin Hub Plaza     The Hub
2   │  Temo’s Cafe            24th Street
3   │  Sisters Coffee Shop    South Mission
4   │  Ocean Beach Cafe       Ocean Beach
5   │  Gambit Lounge          Hayes Valley
6   │  Jane on Larkin         Lower Polk
7   │  Another Cafe           Lower Nob Hill
8   │  Driftwood              West SoMa
9   │  The Social Study       Fillmore
10  │  TBD                    Lower Height          # Never actually used
11  │  Manny's                Northern Mission
12  │  Grata Wines            South Bayview
13  │  Ocean Ale House        Ingleside
14  │  All Good Pizza         Bayview

```

```{python}
cleanups = pl.read_csv("data/cleanup_events.csv", schema_overrides={"count": pl.Float32}).select(
  pl.col("location"),
  pl.col("time").str.to_datetime("%B %d, %Y, %I:%M %p"),
  pl.col("count").ceil().alias("participant_count")
).join(
  pl.read_csv("data/cleanup_locations.csv").select(
    pl.col("location"),
    pl.col("lon"),
    pl.col("lat"),
  ),
  how="left",
  on="location"
)
```

# Feature extraction

```{python}
# Convert to geopandas, calculate distances, time since most recent/nearest cleanup
# Should just threshold cleanup distance, e.g. find most recent cleanup that was within 1/4 mile of observation.
# join observation on nearest cleanup name
#   take most recent cleanup before observation

# add circles to observations, radius 1/4 mile
#   intersect with cleanups
#   within intersection, take most recent cleanup before observation

import geopandas as gp
import pandas as pd

cleanups_gp = gp.GeoDataFrame(cleanups.to_pandas(), geometry=gp.points_from_xy(cleanups["lon"], cleanups["lat"]), crs="EPSG:3857")
observations_gp = gp.GeoDataFrame(observations.to_pandas(), geometry=gp.points_from_xy(observations["lon"], observations["lat"]), crs="EPSG:3857")

joined = observations_gp.sjoin_nearest(cleanups_gp, distance_col="distance")

joined = pl.DataFrame(joined.drop(columns="geometry"))

examples = (
  joined
    .filter(pl.col("time") < pl.col("evaluation_date"))
    .group_by(["object_id", "route_id"])
    .agg(pl.all().sort_by(pl.col("time")).last())
    .select(
      pl.col("distance"),
      pl.col("location"),
      pl.col("participant_count"),
      pl.col("sidewalk_litter"),
      pl.col("evaluation_date").alias("evaluation_time"),
      pl.col("time").alias("cleanup_time"),
      pl.col("lon_left").alias("observation_lon"),
      pl.col("lat_left").alias("observation_lat"),
      pl.col("lon_right").alias("cleanup_lon"),
      pl.col("lat_right").alias("cleanup_lat"),
      (pl.col("evaluation_date") - pl.col("time")).alias("time_delta")
    )
)
examples_gp = gp.GeoDataFrame(examples.to_pandas())
examples_gp["observation_geom"] = gp.points_from_xy(examples_gp["observation_lon"], examples_gp["observation_lat"])
examples_gp["cleanup_geom"] = gp.points_from_xy(examples_gp["cleanup_lon"], examples_gp["cleanup_lat"])
examples_gp["geometry"] = examples_gp["observation_geom"].shortest_line(examples_gp["cleanup_geom"])
examples_gp["time_delta"] = examples_gp["time_delta"].dt.total_seconds() / (60 * 60 * 24)
examples_gp = examples_gp.drop(["evaluation_time", "cleanup_time", "observation_geom", "cleanup_geom"], axis=1)
examples = pl.DataFrame(pd.DataFrame(examples_gp.drop("geometry", axis=1)))
```

```{python}
ojs_define(examples=examples_gp[examples_gp["distance"] <= .01].to_json())
```

```{ojs}
mutable selected_precinct = {}
selected_precinct
```

```{ojs}
div = document.createElement("div");
```

```{ojs}
//| output: false
mapboxgl = require("mapbox-gl")
div.style = "height: 1000px; overflow: hidden;";

map = new mapboxgl.Map({
  container: div,
  accessToken: "pk.eyJ1IjoiZHBybyIsImEiOiJjamhrNG03N2gweHFrMzdxb3A3ZXc2MDd2In0.FC_fu8WvE7POuOeYMUGHyg",
  center: [-122.4380, 37.7500],
  zoom: 12.2,
});

map.on('load', () => {
        // Precinct data
        map.addSource('examples-source', {
            'type': 'geojson',
            'data': JSON.parse(examples),
        });
        map.addLayer(
            {
                'id': 'examples-labels',
                'type': 'symbol',
                'source': 'examples-source',
                'layout': {
                  "text-field": "{distance}",
                }
            },
        );
        map.addLayer(
            {

                'id': 'examples',
                'type': 'line',
                'source': 'examples-source',
                'layout': {
                    'line-join': 'round',
                    'line-cap': 'round'
                },
                'paint': {
                    'line-color': '#BF93E4',
                    'line-width': 5
                }
            },
        );
});
```
```{python}
with pl.Config(tbl_rows=-1):
  print(
  examples
    .filter(pl.col("distance") < .01)
    .filter(pl.col("time_delta") <= 1 * 7)
    ["location"].value_counts().sort("count")
  )
```

```{python}
 
import seaborn as sns

sns.scatterplot(
  data=examples,
  x="distance",
  y="time_delta",
  hue="sidewalk_litter",
)
```

```{python}
sns.displot(
  data=(
    examples
    .with_columns(
      (pl.col("distance") <= .01).alias("close_to_cleanup"),
    )
    .filter(pl.col("distance") <=.01)
    .filter(pl.col("time_delta") <= 1 * 7)
    .filter(pl.col("location").is_in([
      # "All Good Pizza",
      # "Grata Wines",
      # "Ocean Beach Cafe",
      # "Ocean Ale House",
      "Sisters Coffee Shop",
      # "Driftwood",
      # "Jane on Larkin",
      # "Gambit Lounge",
      # "Another Cafe",
      # "Manny's",
      # "The Social Study",
    ]))
  ),
  kind="kde",
  x="time_delta",
  hue="sidewalk_litter",
  row="location",
  multiple="fill",
  clip=[0, 1 * 7],
  rug=True,
  rug_kws={'color':'#0426d0', 'edgecolor':'#00dbff'}
)
```

```{python}
import statsmodels.formula.api as smf

model = smf.ols(
  'sidewalk_litter ~ time_delta + distance',
  data=(
    examples
      .filter(pl.col("time_delta") <= 2 * 7)
      .filter(pl.col("location").is_in([
        # "All Good Pizza",
        # "Grata Wines",
        # "Ocean Beach Cafe",
        # "Ocean Ale House",
        "Sisters Coffee Shop",
        # "Driftwood",
        # "Jane on Larkin",
        # "Gambit Lounge",
        # "Another Cafe",
        # "Manny's",
        # "The Social Study",
      ]))
  )
).fit()
model.summary()
```

:::{.callout-important}
We see that, for locations within the area of a cleanup, the **level of litter goes up by roughly a third point per week since the last cleanup**.
:::
